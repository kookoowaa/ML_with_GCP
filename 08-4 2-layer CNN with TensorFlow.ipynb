{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "># TensorFlow로 2층 CNN 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">## CNN의 전체적인 구성\n",
    "\n",
    "- 합성곱과 풀링층을 각각 2층, 그리고 전결합층과 드롭아웃층을 포함한 CNN 구성\n",
    "- 여기에서 사용한 층 수와 필터 수는 앞선 개발자들이 연구하여 얻은 값으로 참조\n",
    "> a. 입력층 (28x28)  \n",
    "> b. 합성곱층: 필터 수 32개 (28x28x32)  \n",
    "> c. 풀링층: 2x2, 스트라이드 2 (14x14x32)  \n",
    "> d. 합성곱층: 필터 수 64개 (14x14x64)  \n",
    "> e. 풀링층: 2x2 스트라이드 2 (7x7x64)  \n",
    "> f. 전결합층 (3136)  \n",
    "> g. 드롭아웃: 50%  \n",
    "> h. 출력층: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    ">## 합성곱층 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 앞서 합성곱층은 단순 예측을 위한 CNN으로 `tf.constant`를 사용하였지만, 이번에는 학습을 진행할 예정으로 `tf.Variable`로 구현\n",
    "- `tf.random_normal`로 정규부포를 활용한 난수로 초기화\n",
    "```python\n",
    "# 첫번째 합성곱층: 5*5 필터, 1채널, 필터수 32\n",
    "w1 = tf.Variable(tf.random_normal([5, 5, 1, 32]))\n",
    "conv1 = tf.nn.conv2d(X_image,\n",
    "                     w1,\n",
    "                     strides = [1, 1, 1, 1],\n",
    "                     padding = \"SAME\")\n",
    "```\n",
    "- 두번째 합성곱층은 입력값의 채널 수가 32로 변경\n",
    "```python\n",
    "# 두번째 합성곱층: 5*5 필터, 32채널, 필터수 64\n",
    "w2 = tf.Variable(tf.random_normal([5, 5, 32, 64]))\n",
    "conv2 = tf.nn.conv2d(conv1,\n",
    "                     w2,\n",
    "                     strides = [1, 1, 1, 1],\n",
    "                     padding = \"SAME\")\n",
    "```\n",
    "\n",
    "- 위 과정은 `tf.layers.conv2d`를 사용하면 쉽게 구현 가능\n",
    "- 가중치는 입력값의 shape에 따라 자동으로 생성\n",
    "- 출력 shape도 `filters` 매개변수 값에 따라 자동 생성\n",
    "- 또한 `tf.layers.conv2d`를 사용하면 합성곱 이후 활성화 함수를 적용하므로, 비선형성을 지닌 CNN에서 성능 향상을 기대할 수 있음 (페널티)\n",
    "```python\n",
    "# 첫번째 합성곱층: 5*5 필터, 1채널, 필터수 32\n",
    "conv1 = tf.layers.conv2d(inputs = X_image,\n",
    "                         filters = 32,\n",
    "                         kernel_size = [5, 5],\n",
    "                         padding = 'SAME',\n",
    "                         activation = tf.nn.relu)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    ">## 풀링층 구현하기\n",
    "\n",
    "- 풀링층도 `tf.layers`를 사용하면 간단하게 구현 가능\n",
    "- 풀링층은 매개변수가 따로 없으므로, 편하게 구현\n",
    "```python\n",
    "pool1 = tf.layers.max_pooling2d(inputs = conv1,\n",
    "                                pool_size = [2, 2],\n",
    "                                strides = 2)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    ">## 전결합층과 드롭아웃층 구현하기\n",
    "\n",
    "- 전결합층 부터는 '이미지 형태'가 아무 상관 없기 때문에 `tf.reshape`를 사용하여 일차원 벡터로 사용\n",
    "- 이미지 크기는 두차례 2x2 풀링으로 28X28 > 14X14 > 7X7로 작아졌으나 필터 수가 64로 증가하여 3136 화소 값을 갖게 됨\n",
    "```python\n",
    "pool2_flat = tf.reshape(pool2,\n",
    "                        [-1, 7*7*64])\n",
    "dense = tf.layers.dense(inputs = pool2_flat,\n",
    "                        units = 1024,\n",
    "                        activation = tf.nn.relu)\n",
    "dropout = tf.layers.dropout(inputs = dense,\n",
    "                            rate = 0.5,\n",
    "                            training = True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    ">## 전체 코드\n",
    "\n",
    "- 2층 CNN의 전체 코드는 아래와 같으며 정답률이 99.23%까지 개선 (전결합층 90~95% 수준)\n",
    "- CNN은 학습에 시간이 매우 오래 걸리며(특히 GPU가 없는 경우), 에포크 수를 낮춰 사전에 예상 시간을 가늠해 보는 것이 효율적\n",
    "- 또한, **Cloud ML Engine** 사용도 리소스 효율 차원에서 검토해 보는 것이 좋음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist/train-images-idx3-ubyte.gz\n",
      "Extracting ./mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting ./mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./mnist/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From <ipython-input-3-96e1f5ea40da>:67: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n",
      "Step: 1099, Loss 0.260841\n",
      "Step: 1099, Loss 0.258715\n",
      "Step: 1099, Loss 0.068271\n",
      "Step: 1099, Loss 0.006807\n",
      "Step: 1099, Loss 0.024814\n",
      "Step: 1099, Loss 0.018018\n",
      "Step: 1099, Loss 0.004972\n",
      "Step: 1099, Loss 0.000446\n",
      "Step: 1099, Loss 0.115407\n",
      "Step: 1099, Loss 0.002886\n",
      "Step: 1099, Loss 0.005884\n",
      "Step: 1099, Loss 0.015561\n",
      "Step: 1099, Loss 0.011737\n",
      "Step: 1099, Loss 0.001439\n",
      "Step: 1099, Loss 0.002101\n",
      "Step: 1099, Loss 0.011948\n",
      "Step: 1099, Loss 0.028400\n",
      "Step: 1099, Loss 0.000694\n",
      "Step: 1099, Loss 0.001600\n",
      "Step: 1099, Loss 0.000020\n",
      "Accuracy: 0.990300\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"./mnist/\")\n",
    "\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "  X = tf.placeholder(tf.float32,\n",
    "                    [None, 784],\n",
    "                    name = 'X')\n",
    "  y = tf.placeholder(tf.float32,\n",
    "                    [None,],\n",
    "                    name = 'y')\n",
    "  \n",
    "  X_image = tf.reshape(X, [-1, 28, 28, 1])\n",
    "  \n",
    "  # 첫번째 합성곱층\n",
    "  conv1 = tf.layers.conv2d(inputs = X_image,\n",
    "                       filters = 32,\n",
    "                       kernel_size = [5, 5],\n",
    "                       padding = 'SAME',\n",
    "                       activation = tf.nn.relu)\n",
    "  \n",
    "  # 첫번째 풀링층\n",
    "  pool1 = tf.layers.max_pooling2d(inputs = conv1,\n",
    "                              pool_size = [2, 2],\n",
    "                              strides = 2)\n",
    "  \n",
    "  # 두번째 합성곱층\n",
    "  conv2 = tf.layers.conv2d(inputs = pool1,\n",
    "                          filters = 64,\n",
    "                          kernel_size = [5, 5],\n",
    "                          padding = 'SAME',\n",
    "                          activation = tf.nn.relu)\n",
    "  \n",
    "  # 두번째 풀링층\n",
    "  pool2 = tf.layers.max_pooling2d(inputs = conv2,\n",
    "                                 pool_size = [2, 2],\n",
    "                                 strides = 2)\n",
    "  \n",
    "  # 전결합층\n",
    "  pool2_flat = tf.reshape(pool2,\n",
    "                         [-1, 7 * 7 * 64])\n",
    "  dense = tf.layers.dense(inputs = pool2_flat,\n",
    "                        units = 1024,\n",
    "                        activation = tf.nn.relu)\n",
    "  \n",
    "  # 드롭아웃층\n",
    "  dropout = tf.layers.dropout(inputs = dense,\n",
    "                             rate = 0.5,\n",
    "                             training = True)\n",
    "  \n",
    "  # 출력층\n",
    "  logits = tf.layers.dense(inputs = dropout,\n",
    "                          units = 10,\n",
    "                          name = 'output')\n",
    "  predict = tf.argmax(logits,\n",
    "                      1)\n",
    "  \n",
    "  # 손실\n",
    "  with tf.name_scope('calc_loss'):\n",
    "    onehot_labels = tf.one_hot(indices = tf.cast(y,\n",
    "                                                tf.int32),\n",
    "                               depth = 10)\n",
    "    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels = onehot_labels,\n",
    "                                                           logits = logits,\n",
    "                                                           name = 'xentropy')\n",
    "    loss = tf.reduce_mean(cross_entropy,\n",
    "                         name = 'xentropy_mean')\n",
    "    \n",
    "  # 손실 최적화\n",
    "  train_op = tf.train.AdamOptimizer(0.0001).minimize(loss)\n",
    "  \n",
    "  # 정답률 구하기\n",
    "  with tf.name_scope('calc_accuracy'):\n",
    "    correct_prediction = tf.equal(tf.argmax(logits,\n",
    "                                           1),\n",
    "                                 tf.argmax(onehot_labels,\n",
    "                                          1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction,\n",
    "                                     tf.float32))\n",
    "    \n",
    "  with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    total_batch = int(mnist.train.num_examples // batch_size)\n",
    "    \n",
    "    for epoch in range(20):\n",
    "      for step in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        _, loss_value = sess.run([train_op, loss],\n",
    "                                feed_dict = {X: batch_xs, y: batch_ys})\n",
    "        \n",
    "      print('Step: %d, Loss %f' %(step, loss_value))\n",
    "      \n",
    "    # 테스트하기\n",
    "    _a = sess.run(accuracy, feed_dict = {X: mnist.test.images, y: mnist.test.labels})\n",
    "    \n",
    "    print('Accuracy: %f' % _a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위의 학습을 진행하는 데 약 한시간 가량의 시간 소요\n",
    "- Cloud ML 엔진 사용을 검토 필요"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
