{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "># 신경망 구현하기\n",
    "\n",
    "#### 데이터\n",
    "- 샘플데이터는 6-2에서 살펴보았던 유방암 데이터 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requesting query... ok.\n",
      "Job ID: e4758ea9-ebda-4cb7-a051-069949e5e3d1\n",
      "Query running...\n",
      "Query done.\n",
      "Cache hit.\n",
      "\n",
      "Retrieving results...\n",
      "Got 569 rows.\n",
      "\n",
      "Total time taken 3.13 s.\n",
      "Finished at 2018-11-06 23:55:11.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "query = 'select * from testdataset.wdbc order by index'\n",
    "dataset = pd.read_gbq(project_id = 'mlwithgcp', query = query)\n",
    "\n",
    "dataset['diagnostic'] = dataset['diagnostic'].apply(lambda X: 0 if X=='M' else 1)\n",
    "dataset.drop('index', axis = 1, inplace = True)\n",
    "X_dataset = dataset.drop('diagnostic', axis = 1).as_matrix()\n",
    "y_dataset = dataset['diagnostic'].as_matrix()\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_dataset, y_dataset,\n",
    "                                                   test_size = 0.2, \n",
    "                                                   random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "#### 입출력 유닛\n",
    "- 샘플신경망은 2개의 레이어, 각각 4개의 유닛으로 구성\n",
    "- 입력층과 출력층은 `placeholder`로 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wdbc 특징량 수 (열)\n",
    "NUM_FEATURES = 30\n",
    "\n",
    "# 특징량과 레이블 입력\n",
    "X = tf.placeholder(np.float32, shape = [None, NUM_FEATURES], name = 'X')\n",
    "y = tf.placeholder(np.float32, shape = [None], name = 'y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ___\n",
    " #### 은닉층\n",
    " - 은닉층 구현은 `tf.layers`로 간단하게 구현 가능\n",
    " - `df.layers.dense`는 전결합층<sub>Fully Connected Layer</sub>이라고 부르는 모든 유닛이 연결된 층을 의미함\n",
    " - 이를 통해 '층의 유닛수'와 '활성화 함수'만 지정하면 구현 가능\n",
    " - `df.layers.dense`는 내부적으로 `variable`을 저장하고 있으며, 학습 처리에 따라 가중치 값을 최적화\n",
    " - `df.layers.dense`를 사용하지 않는 경우 각각의 요소들을 명시적으로 정의하여 구현하는 것도 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_UNITS_H1 = 4\n",
    "NUM_UNITS_H2 = 4\n",
    "\n",
    "# 은닉층\n",
    "hidden1 = tf.layers.dense(inputs = X,\n",
    "                         units = NUM_UNITS_H1,\n",
    "                         activation = tf.nn.relu,\n",
    "                         name = 'hidden1')\n",
    "hidden2 = tf.layers.dense(inputs = hidden1,\n",
    "                         units = NUM_UNITS_H2,\n",
    "                         activation = tf.nn.relu,\n",
    "                         name = 'hidden2')\n",
    "\n",
    "\n",
    "#### df.layers.dense를 사용하지 않는 경우 ####\n",
    "###########################################\n",
    "# 은닉층1\n",
    "w1 = tf.Variable(tf.truncated_normal([NUM_FEATURES, NUM_UNITS_H1], stddev = 0.1), name = 'w1')\n",
    "b1 = tf.Variable(tf.zeros([NUM_UNITS_H1]), name = 'b1')\n",
    "hidden1 = tf.nn.relu(tf.matmul(X, w1) + b1)\n",
    "\n",
    "# 은닉층2\n",
    "w2 = tf.Variable(tf.truncated_normal([NUM_UNITS_H1, NUM_UNITS_H2], stddev = 0.1), name = 'w2')\n",
    "b2 = tf.Variable(tf.zeros([NUM_UNITS_H2]), name = 'b2')\n",
    "hidden2 = tf.nn.relu(tf.matmul(X, w1) + b2)\n",
    "# truncated_normal은 음수부분을 절단한 절단 정규분포로 임의의 초기값 전달을 위해 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위 코드는 별도로 유닛 수를 설정하지 않음\n",
    "- 대신 가중치의 shape에 따라 행렬곱으로 복수의 유닛 계산을 수행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "#### 출력층\n",
    "- 출력값은 레이블이 2개로, 2개의 유닛으로 출력층을 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 클래스 수\n",
    "NUM_CLASSES = 2\n",
    "\n",
    "# 출력층\n",
    "logits = tf.layers.dense(inputs = hidden2,\n",
    "                         units = NUM_CLASSES,\n",
    "                         name = 'output')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "#### 손실함수\n",
    "\n",
    "- 출력결과와 정답레이블(one-hot-vector)로 손실 계산\n",
    "- `softmax_cross_entropy_with_logits`로 softmax와 손실 계산을 동시에 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 손실 (softmax + entropy)\n",
    "one_hot_labels = tf.one_hot(indices = tf.cast(y, tf.int32),\n",
    "                           depth = NUM_CLASSES)\n",
    "\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(labels = one_hot_labels,\n",
    "                                                        logits = logits,\n",
    "                                                        name = 'xentropy')\n",
    "\n",
    "loss = tf.reduce_mean(cross_entropy, name = 'xentropy_mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 손실 최소화\n",
    "train_op = tf.train.AdamOptimizer(0.01).minimize(loss)\n",
    "\n",
    "# 정답률 계산\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1),\n",
    "                             tf.argmax(one_hot_labels, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "#### 계산\n",
    "- 계산그래프가 완성되었고, `train_op`와 `loss`, `feed_dict:....`를 입력값으로 훈련 수행\n",
    "- 1000회 반복 후 테스트 전용 데이터로 정답률 계산\n",
    "- 입력층의 shape을 변경하면 다른 데이터도 학습 및 활용이 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0, Loss: 20.874336\n",
      "Step: 100, Loss: 0.336367\n",
      "Step: 200, Loss: 0.237237\n",
      "Step: 300, Loss: 0.208793\n",
      "Step: 400, Loss: 0.188213\n",
      "Step: 500, Loss: 0.173930\n",
      "Step: 600, Loss: 0.163369\n",
      "Step: 700, Loss: 0.154654\n",
      "Step: 800, Loss: 0.146571\n",
      "Step: 900, Loss: 0.138466\n",
      "Accuray: 0.956140\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "  sess.run(tf.global_variables_initializer())\n",
    "  \n",
    "  for step in range(1000):\n",
    "    _, loss_value = sess.run([train_op, loss],\n",
    "                            feed_dict = {X:X_train, y:y_train})\n",
    "    \n",
    "    if step % 100 ==0:\n",
    "      print('Step: %d, Loss: %f' % (step, loss_value))\n",
    "\n",
    "  _a = sess.run(accuracy, feed_dict ={X:X_test, y:y_test})\n",
    "  print('Accuray: %f' % _a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    ">## 정리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_FEATURES = 30\n",
    "NUM_UNITS_H1 = 4\n",
    "NUM_UNITS_H2 = 4\n",
    "NUM_CLASSES = 2\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "  # 유닛\n",
    "  X = tf.placeholder(np.float32, shape = [None, NUM_FEATURES], name = 'X')\n",
    "  y = tf.placeholder(np.float32, shape = [None], name = 'y')\n",
    "  \n",
    "  # 은닉층\n",
    "  hidden1 = tf.layers.dense(inputs = X,\n",
    "                           units = NUM_UNITS_H1,\n",
    "                           activation = tf.nn.relu,\n",
    "                           name = 'hidden1')\n",
    "  hidden2 = tf.layers.dense(inputs = hidden1,\n",
    "                           units = NUM_UNITS_H2,\n",
    "                           activation = tf.nn.relu,\n",
    "                           name = 'hidden2')\n",
    "\n",
    "  # 출력층\n",
    "  logits = tf.layers.dense(inputs = hidden2,\n",
    "                           units = NUM_CLASSES,\n",
    "                           name = 'output')\n",
    "\n",
    "  # 손실 (softmax + entropy)\n",
    "  one_hot_labels = tf.one_hot(indices = tf.cast(y, tf.int32),\n",
    "                             depth = NUM_CLASSES)\n",
    "\n",
    "  cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(labels = one_hot_labels,\n",
    "                                                          logits = logits,\n",
    "                                                          name = 'xentropy')\n",
    "\n",
    "  loss = tf.reduce_mean(cross_entropy, name = 'xentropy_mean')\n",
    "\n",
    "\n",
    "  # 손실 최소화\n",
    "  train_op = tf.train.AdamOptimizer(0.01).minimize(loss)\n",
    "\n",
    "  # 정답률 계산\n",
    "  correct_prediction = tf.equal(tf.argmax(logits, 1),\n",
    "                                 tf.argmax(one_hot_labels, 1))\n",
    "  accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "  \n",
    "  # 실행\n",
    "  with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "  \n",
    "    for step in range(1000):\n",
    "      _, loss_value = sess.run([train_op, loss],\n",
    "                              feed_dict = {X:X_train, y:y_train})\n",
    "    \n",
    "      if step % 100 ==0:\n",
    "        print('Step: %d, Loss: %f' % (step, loss_value))\n",
    "\n",
    "    _a = sess.run(accuracy, feed_dict ={X:X_test, y:y_test})\n",
    "    print('Accuray: %f' % _a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
